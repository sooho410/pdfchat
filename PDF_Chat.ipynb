{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KptNpc_ZB-m"
      },
      "source": [
        "## 1. 환경구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "F12oAcvrZBKS",
        "outputId": "63db6e10-8073-4fb3-af2f-171ebc720feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlangchain: version: 0.1.8\\nopenai: openai api 사용\\nchromadb: vectorstore\\ntiktoken: OpenAI 모델이 사용하는 토크나이저(Tokenizer)\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cf. q옵션은 더 적은 출력제공 - WARNING.ERROR/CRITICAL로깅 수준\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "# !pip install langchain unstructured pypdf pdf2image docx2txt pdfminer langchainhub python-dotenv openai chromadb tiktoken\n",
        "!pip -q install langchain pypdf pdfminer openai chromadb tiktoken rank_bm25\n",
        "\n",
        "'''\n",
        "langchain: version: 0.1.8\n",
        "openai: openai api 사용\n",
        "chromadb: vectorstore\n",
        "tiktoken: OpenAI 모델이 사용하는 토크나이저(Tokenizer)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWCQo-rpBDTy"
      },
      "source": [
        "## 2. opanai_api 사용을 위한 환경변수 설정 및 google drive mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADy-FE7UBO7Q",
        "outputId": "4a0ab248-abd4-49c2-ba3e-6c328b7a3df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/langchain_pdf\n"
          ]
        }
      ],
      "source": [
        "# OPENAI API키\n",
        "import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"API KEY입력\"\n",
        "\n",
        "# google drive mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('google colab pdf 파일 경로를 입력하세요')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j_hQjOxZBIH"
      },
      "outputs": [],
      "source": [
        "# 토큰 정보로드\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnXxyXbkaDRo"
      },
      "source": [
        "## 3. Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29I9mbsbBiOB",
        "outputId": "037a43c7-3c3a-4777-cdcf-24204ff8a2b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "* 챗봇 구동을 위한 프로세스\n",
        "\n",
        "1. 데이터 로드\n",
        "2. 데이터 분할\n",
        "3. 저장 및 검색\n",
        "4. 프롬프트 템플릿 생성\n",
        "5. Chain 생성\n",
        "6. Chatbot 구현 및 실행\n",
        "'''\n",
        "\n",
        "### 1. 데이터 로드\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf\")\n",
        "document = loader.load()\n",
        "\n",
        "\n",
        "### 2. 데이터 분할\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(document)\n",
        "\n",
        "### 3. 저장 및 검색\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# 임베딩\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# Chroma DB 에 저장\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "# retriever 생성 (검색기)\n",
        "retriever = docsearch.as_retriever()\n",
        "\n",
        "### 4. 프롬프트 템플릿 생성\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "customized_prompt= ChatPromptTemplate(input_variables=['question', 'context'],\n",
        "                                      output_parser=None,\n",
        "                                      partial_variables={},\n",
        "                                      messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'context'],\n",
        "                                                                                                 output_parser=None,\n",
        "                                                                                                 partial_variables={},\n",
        "                                                                                                 template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
        "                                                                                                 template_format='f-string',\n",
        "                                                                                                 validate_template=True), # 유효성검사\n",
        "                                                                           additional_kwargs={})])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 5. Chain 생성\n",
        "# LLM 모델 (ChatGPT)\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=0)\n",
        "\n",
        "# chain 생성\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "# pipe operator를 활용한 체인 생성\n",
        "llm_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | customized_prompt\n",
        "    | llm\n",
        ")\n",
        "\n",
        "\n",
        "### 6. Chatbot 구현 및 실행\n",
        "\n",
        "def pdf_chatbot(question):\n",
        "  message_q = question\n",
        "  message_a = llm_chain.invoke(question).content\n",
        "\n",
        "  message_qa = f\"\"\"\n",
        "  Q.\\n\n",
        "  {message_q} \\n\\n\n",
        "  A.\\n\n",
        "  {message_a}\n",
        "  \"\"\"\n",
        "  return message_qa\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pdf_chatbot_rqa(question):\n",
        "  from langchain.chains import RetrievalQA\n",
        "  # RetrevalQA\n",
        "  qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                   chain_type=\"stuff\",\n",
        "                                   retriever=docsearch.as_retriever(\n",
        "                                       search_type=\"mmr\",\n",
        "                                       search_kwargs={\"k\":3,\"fetch_k\":10}\n",
        "                                       ),\n",
        "                                   return_source_documents=True)\n",
        "  query = question\n",
        "  answer_sub = qa(query)\n",
        "\n",
        "  # 답변에 추가할 내용\n",
        "  message_q = answer_sub['query']\n",
        "  message_a = answer_sub['result']\n",
        "  source_doc = answer_sub['source_documents']\n",
        "  ref_page_lst = list(set([str(source_doc[k].metadata['page']+1)+\"페이지\" for k in range(len(source_doc))]))\n",
        "  ref_doc_lst = list(set([source_doc[k].metadata['source'] for k in range(len(source_doc))]))\n",
        "\n",
        "  # 최종답변\n",
        "  message_qa = f'''\n",
        "  Q.\n",
        "  {message_q}\n",
        "  A.\n",
        "  {message_a}\n",
        "\n",
        "  Reference.\n",
        "  documents: {\",\".join(ref_doc_lst)}\n",
        "  pages: {\",\".join(ref_page_lst)}\n",
        "  '''\n",
        "\n",
        "  return message_qa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiIOCrkVW_At",
        "outputId": "d4f02ce1-f941-479f-9308-563dfe7eef12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x793dc8289060>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x793dc828a500>, model_name='gpt-4-0613', temperature=0.0, openai_api_key='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', openai_proxy='')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIndKCrZJ03e",
        "outputId": "28e04a6d-df4f-42e2-f8e4-d5a07e15bdce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7d26ec125150>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7d26d6914040>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=0)\n",
        "\n",
        "# chain 생성\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "# pipe operator를 활용한 체인 생성\n",
        "llm_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | customized_prompt\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZwzNx5bQk89",
        "outputId": "092494ed-65de-4724-d732-e19102a76fb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='올여름의불타는LLM세상도불타고나도불타고\\n신정규래블업주식회사@inureyes2023년8월2일', metadata={'source': '/content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf', 'page': 0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x9atTskkw1Z",
        "outputId": "ba7d8e5f-c081-4cf1-af0a-b6ed33484ffa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7c606e7b5060>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7c606e5f90f0>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqXlytRAY81r",
        "outputId": "6a9bc80c-3f07-4009-f859-d6e0befe283e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='•거대언어모델–실용화할타이밍은아님–“Attentionis all you need” (Google, 2017)–Stable Diffusion 과ChatGPT가가져가버린것✓나도한입만…✓이제K-나올차례✓그런데전세계에서다나오는중이다.A-,B-,C-,…J-…•예:BritGPT–힌튼교수의구글퇴사(5월2일)✓하고싶은말이많아퇴사하셨다고..✓“다들너무뛰기만하는거아니냐?”Sizedoesmatter\\nLLM!\\n진심으로그길을?66', metadata={'page': 65, 'source': '/content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf'}),\n",
              " Document(page_content='감사합니다contact@lablup.comhttps://www.facebook.com/lablupIncLablupInc.https://www.lablup.comBackend.AI  https://www.backend.aiBackend.AI GitHub https://github.com/lablup/backend.aiBackend.AI Cloud https://cloud.backend.ai\\n88', metadata={'source': '/content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf', 'page': 87}),\n",
              " Document(page_content='•2023년7월21일앤드류응세미나중질문드린내용•이더큰팜만들고계속큰모델만드는사이클은언제끝날까?•답변–현재AI 분야는거의인간이할것이없다.고가의GPU를엄청나게산다.왜?–유저한명, 한시간당몇센트의인퍼런스비용으로텍스트생성가능.엄청비싸긴한데, 규모가되면말이된다.–아직GPU를갈아넣는것이사람보다싸다.이비용불균형이유지되는동안사이클이계속될것이다.마치며\\n87', metadata={'source': '/content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf', 'page': 86})]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.retrievers  import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(texts)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "chroma_vector = Chroma.from_documents(texts, embeddings)\n",
        "chroma_retriever = chroma_vector.as_retriever(search_kwargs={'k':2})\n",
        "\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "                    retrievers = [bm25_retriever,chroma_retriever]\n",
        "                    , weight = {0.5,0.5})\n",
        "\n",
        "docs = ensemble_retriever.invoke(\"구글 바드는?\")\n",
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_BWvn6qY8zM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaRXhLsmY8wq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8V9_FMGY8uB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGAjofMzDIBm"
      },
      "source": [
        "## 4. 챗봇 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaqcIgk97Ybv",
        "outputId": "951199f2-7efd-4380-a3a0-3c7ebb2db940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Q.\n",
            "\n",
            "  모델 훈련 분산 처리 목표는 무엇인가? \n",
            "\n",
            "\n",
            "  A.\n",
            "\n",
            "  모델 훈련 분산 처리의 목표는 빠른 훈련 속도, 최소한의 추가적인 수고, 그리고 재현용이성을 달성하는 것입니다. 이를 통해 훈련 속도를 높이고 코드 수정을 최소화하여 스케일을 달성하며, 낮은 시스템의 의존성을 유지합니다.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(pdf_chatbot(\"모델 훈련 분산 처리 목표는 무엇인가?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "hFYnJPRW7YTG",
        "outputId": "a60eff99-5fe2-4984-ecdc-09e809490f8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n  Q.\\n\\n  체크포인트 기반 추가학습의 문제점은? \\n\\n\\n  A.\\n\\n  체크포인트 기반 추가학습의 문제점은 원 모델 훈련이 요구했던 연산 자원 종류와 연산 자원량이 필요하다는 것입니다. 자원이 부족할 경우 훈련 속도가 느려지거나 모델을 GPU 메모리에 올릴 수 없는 경우가 발생할 수 있습니다. 또한, CUDA / ROCm 호환성 문제가 발생하거나 혼합 정밀도를 사용하는 모델에서 문제가 발생할 수 있습니다.\\n  '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_chatbot(\"체크포인트 기반 추가학습의 문제점은?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "oI1yiMzF8ZXQ",
        "outputId": "9855391a-b9c1-4e20-9eb5-a78b5dea5fce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n  Q.\\n\\n  llama사태를 요약해줘 \\n\\n\\n  A.\\n\\n  Llama 사태는 Meta가 Llama 모델을 공개하고 이를 연구 목적으로 사용할 수 있도록 했던 사건입니다. 그러나 이후에 모델의 weight와 checkpoint가 토렌트를 통해 유출되었습니다. 이 유출로 인해 라이선스 위반 문제가 발생하였고, Meta는 이를 차단하려고 했지만 완전히 막기 어려웠습니다.\\n  '"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_chatbot(\"llama사태를 요약해줘\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47jdsaWR2Mk4",
        "outputId": "f3d6fe17-bf8e-4c76-8033-6e5bf41b2721"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Q.\n",
            "  체크포인트 기반 추가학습의 문제점은?\n",
            "  A.\n",
            "  체크포인트 기반 추가학습의 문제점은 원 모델 훈련이 요구했던 연산 자원 종류/연산 자원량이 필요하다는 점입니다. 이로 인해 자원이 적을 경우 훈련 속도가 느려지거나 모델을 GPU 메모리에 올릴 수 없는 경우가 발생할 수 있습니다. 또한, 최소한 체크포인트 적재가 가능한 만큼의 GPU 메모리가 필요하며, CUDA / ROCm 호환성 문제가 발생할 수 있습니다. 특히 혼합 정밀도를 사용하는 모델에서 이러한 문제가 자주 발생합니다.\n",
            "\n",
            "  Reference.\n",
            "  documents: /content/langchain_pdf/MyDrive/20230802_Burning_LLM_of_this_Summer.pdf\n",
            "  pages: 60페이지,66페이지,21페이지\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(pdf_chatbot_rqa(\"체크포인트 기반 추가학습의 문제점은?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45Dn44cL8YJR"
      },
      "outputs": [],
      "source": [
        "pdf_chatbot_rqa(\"분산 훈련 도구들의 종류는?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M9eHVxb8JZ4"
      },
      "outputs": [],
      "source": [
        "pdf_chatbot_rqa(\"체크포인트 기반 추가학습의 문제점은?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jQDnarc8OGu"
      },
      "outputs": [],
      "source": [
        "pdf_chatbot_rqa(\"llama의 문제점은?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
